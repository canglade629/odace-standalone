"""SQL-based pipeline class using DuckDB for silver transformations."""
import duckdb
import pandas as pd
from typing import Dict, Any, List
from abc import abstractmethod
from app.pipelines.base import BaseSilverPipeline
from deltalake import DeltaTable
import google.auth
import google.auth.transport.requests
import logging
import re

logger = logging.getLogger(__name__)


def get_gcs_storage_options() -> Dict[str, str]:
    """
    Get storage options for Delta Lake to access GCS using Application Default Credentials.
    
    Returns:
        Dictionary with GCS authentication token
    """
    try:
        creds, project_id = google.auth.default()
        logger.info(f"Got Google Auth credentials for project: {project_id}")
        
        # Refresh token if needed
        if not creds.token:
            logger.info("Token not available, refreshing...")
            request = google.auth.transport.requests.Request()
            creds.refresh(request)
            logger.info("Token refreshed successfully")
        
        token = creds.token
        if token:
            logger.info(f"GCS OAuth token available (length: {len(token)})")
            # Note: deltalake 0.15.2 may not support GCS OAuth tokens
            # Returning empty dict to use Application Default Credentials
            logger.warning("DeltaLake 0.15.2 may not support GS_OAUTH_TOKEN - using ADC")
            return {}
        else:
            logger.error("Token is None after refresh attempt")
            return {}
    except Exception as e:
        logger.error(f"Failed to get GCS credentials: {e}", exc_info=True)
        # Return empty dict - DeltaTable will try to use default auth
        return {}


class SQLSilverPipeline(BaseSilverPipeline):
    """
    Base class for SQL-based silver transformations using DuckDB.
    
    Hybrid approach:
    1. Loads Delta tables from GCS using deltalake library (which handles GCS auth)
    2. Registers DataFrames in DuckDB
    3. Executes SQL query against registered tables
    """
    
    @abstractmethod
    def get_sql_query(self) -> str:
        """
        Get SQL query for transformation.
        
        Reference bronze/silver tables that will be loaded, e.g.:
        - SELECT * FROM bronze_geo
        - SELECT * FROM silver_geo
        
        The tables are automatically loaded from GCS paths generated by:
        - self.settings.get_bronze_path("table_name")
        - self.settings.get_silver_path("table_name")
        
        Returns:
            SQL query string
        """
        pass
    
    def get_source_tables(self) -> List[str]:
        """
        Not needed for SQL pipelines - tables extracted from SQL query.
        
        Returns:
            Empty list
        """
        return []
    
    def _extract_table_references(self, sql: str) -> Dict[str, str]:
        """
        Extract table references from SQL query.
        
        Looks for patterns like bronze_geo, silver_geo, etc.
        
        Args:
            sql: SQL query string
            
        Returns:
            Dict mapping table aliases to GCS paths
        """
        import re
        
        paths = {}
        
        # Pattern to find table references like bronze_<name> or silver_<name>
        pattern = r'\b(bronze|silver)_(\w+)\b'
        
        # Find all unique matches
        matches = set(re.findall(pattern, sql, re.IGNORECASE))
        
        for layer, table_name in matches:
            layer_lower = layer.lower()
            table_alias = f"{layer_lower}_{table_name}"
            
            # Generate the GCS path
            if layer_lower == "bronze":
                path = self.settings.get_bronze_path(table_name)
            else:  # silver
                path = self.settings.get_silver_path(table_name)
            
            paths[table_alias] = path
            logger.debug(f"Extracted table reference: {table_alias} -> {path}")
        
        return paths
    
    def transform(self, source_data: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        Execute SQL transformation using DuckDB.
        
        1. Extracts table references from the SQL query
        2. Loads Delta tables from GCS using deltalake library
        3. Registers DataFrames in DuckDB
        4. Executes SQL query
        
        Args:
            source_data: Not used - SQL queries Delta tables directly
            
        Returns:
            Transformed DataFrame
        """
        logger.info(f"Executing SQL transformation for {self.get_name()}")
        
        # Get SQL query
        sql = self.get_sql_query()
        logger.debug(f"SQL Query:\n{sql}")
        
        # Extract table references from SQL
        table_paths = self._extract_table_references(sql)
        
        if not table_paths:
            raise ValueError(f"No table references found in SQL query. Use bronze_<name> or silver_<name> syntax.")
        
        # Load Delta tables into DataFrames
        # Get GCS authentication token
        storage_options = get_gcs_storage_options()
        
        loaded_tables = {}
        for alias, path in table_paths.items():
            logger.info(f"Loading Delta table: {alias} from {path}")
            try:
                # Use explicit GCS OAuth token for authentication
                dt = DeltaTable(path, storage_options=storage_options)
                df = dt.to_pandas()
                loaded_tables[alias] = df
                logger.info(f"Loaded {len(df)} rows from {alias}")
            except Exception as e:
                logger.error(f"Failed to load Delta table {alias} from {path}: {e}")
                logger.error(f"Storage options were: {list(storage_options.keys())}")
                raise
        
        # Execute query with DuckDB
        try:
            # Create in-memory DuckDB connection
            conn = duckdb.connect(":memory:")
            
            # Register DataFrames as DuckDB tables
            for alias, df in loaded_tables.items():
                conn.register(alias, df)
                logger.debug(f"Registered {alias} with {len(df)} rows in DuckDB")
            
            # Execute query
            result_df = conn.execute(sql).df()
            logger.info(f"SQL query returned {len(result_df)} rows")
            
            # Close connection
            conn.close()
            
            return result_df
        except Exception as e:
            logger.error(f"SQL execution failed: {e}")
            logger.error(f"Query was:\n{sql}")
            raise
